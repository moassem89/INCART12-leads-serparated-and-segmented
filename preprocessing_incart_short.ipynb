{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "preprocessing-incart-corrected.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkE_OQ0W6bwf",
        "outputId": "408055dc-6879-49d8-9b51-68b35e191dcb"
      },
      "source": [
        "pip install wfdb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wfdb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/a0/922d06ec737e219a9f45545432842e68a84e8b52f292704056eea1d35e41/wfdb-3.1.1.tar.gz (113kB)\n",
            "\r\u001b[K     |██▉                             | 10kB 18.2MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 20kB 25.8MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 30kB 19.5MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 40kB 18.5MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 51kB 14.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 61kB 14.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 71kB 12.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 81kB 13.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 92kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 102kB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 112kB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 12.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2016.8.2 in /usr/local/lib/python3.6/dist-packages (from wfdb) (2020.12.5)\n",
            "Requirement already satisfied: chardet>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from wfdb) (3.0.4)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.6/dist-packages (from wfdb) (0.10.0)\n",
            "Requirement already satisfied: idna>=2.2 in /usr/local/lib/python3.6/dist-packages (from wfdb) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from wfdb) (0.17.0)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from wfdb) (1.3.1)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wfdb) (3.2.2)\n",
            "Collecting mne>=0.18.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4d/0e/6448521738d3357c205795fd5846d023bd7935bb83ba93a1ba0f7124205e/mne-0.21.2-py3-none-any.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 13.1MB/s \n",
            "\u001b[?25hCollecting nose>=1.3.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 71.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.10.1 in /usr/local/lib/python3.6/dist-packages (from wfdb) (1.18.5)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from wfdb) (1.1.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.4 in /usr/local/lib/python3.6/dist-packages (from wfdb) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.6/dist-packages (from wfdb) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.6/dist-packages (from wfdb) (2018.9)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.6/dist-packages (from wfdb) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from wfdb) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from wfdb) (1.4.1)\n",
            "Requirement already satisfied: six>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from wfdb) (1.15.0)\n",
            "Requirement already satisfied: sklearn>=0.0 in /usr/local/lib/python3.6/dist-packages (from wfdb) (0.0)\n",
            "Collecting threadpoolctl>=1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: urllib3>=1.22 in /usr/local/lib/python3.6/dist-packages (from wfdb) (1.24.3)\n",
            "Building wheels for collected packages: wfdb\n",
            "  Building wheel for wfdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wfdb: filename=wfdb-3.1.1-cp36-none-any.whl size=117829 sha256=76b317a8bc5685143879e1cf7b8f899456d178cba87cb7a3aba5a1e1ad1d1256\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/d0/c1/90538d266ccba2d1076fbc9970192c7ea1a09c99df3e65c69b\n",
            "Successfully built wfdb\n",
            "Installing collected packages: mne, nose, threadpoolctl, wfdb\n",
            "Successfully installed mne-0.21.2 nose-1.3.7 threadpoolctl-2.1.0 wfdb-3.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg4OyqJz6mBt",
        "outputId": "bb86e9c4-7276-4429-9f4f-270ddd4fc6be"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "YxtDkcla6ZtZ"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from scipy.signal import argrelextrema\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "import wfdb"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "ru7Hkf556Ztg"
      },
      "source": [
        "data_path = '/content/drive/MyDrive/Colab Notebooks/INCART12/I'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4rMDZJL16Zth"
      },
      "source": [
        "pts = [] #patients ids\n",
        "\n",
        "for pt in range(1,76):\n",
        "    if pt <10:\n",
        "        pts.append('0'+str(pt))\n",
        "    else:\n",
        "        pts.append(str(pt))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6wYMMMz6Zth"
      },
      "source": [
        "\n",
        "Let's load all the annotations and see the distribution of heart beat types across all files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ol-gHYdl6Zth"
      },
      "source": [
        "df = pd.DataFrame()\n",
        "\n",
        "for pt in pts:\n",
        "\n",
        "    annotation = wfdb.rdann(data_path + pt, 'atr')\n",
        "    sym = annotation.symbol\n",
        "    \n",
        "    values, counts = np.unique(sym, return_counts=True)\n",
        "    df_sub = pd.DataFrame({'sym':values, 'val':counts, 'pt':[pt]*len(counts)})\n",
        "    df = pd.concat([df, df_sub],axis = 0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BWcW02tF6Zti"
      },
      "source": [
        "#Let's write a function for loading a single patient's signals and annotations. Note the annotation values are the indices of the signal array.\n",
        "\n",
        "def load_ecg(file):\n",
        "\n",
        "    record = wfdb.rdrecord(file)\n",
        "    # load the annotation\n",
        "    annotation = wfdb.rdann(file, 'atr')\n",
        "    \n",
        "    # extract the signal\n",
        "    p_signal = record.p_signal\n",
        "    \n",
        "    # verify frequency is 257\n",
        "    #assert record.fs == 257, 'sample freq is not 360'\n",
        "    \n",
        "    # extract symbols and annotation index\n",
        "    atr_sym = annotation.symbol\n",
        "    atr_sample = annotation.sample\n",
        "    \n",
        "    return p_signal, atr_sym, atr_sample"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fS3zsdrD6Zto"
      },
      "source": [
        "def make_dataset(num_cols,pts,lead, num_sec, fs):\n",
        "    # function for making dataset ignoring non-beats\n",
        "    # input:\n",
        "    # a predefined width enough to capture any [R, R+1.2*T] interval\n",
        "    # pts - list of patients\n",
        "    # num_sec = number of seconds to include before and after the beat\n",
        "    # fs = frequency (we take 257Hz as the data were collected accordingly)\n",
        "    # output: \n",
        "    #   X_all = signal (nbeats , num_sec * fs columns)\n",
        "    #   Y_all = beat annotation symbol (nbeats,1)\n",
        "    \n",
        "    # initialize numpy arrays\n",
        "    \n",
        "    X_all = np.zeros((1,num_cols))\n",
        "    Y_all = []\n",
        "    \n",
        "    window_width = fs*num_sec\n",
        "    \n",
        "    for pt in pts:\n",
        "        p_signal, atr_sym, atr_sample = load_ecg(data_path + str(pt))\n",
        "        \n",
        "        # grab a lead signal\n",
        "        p_signal_lead = p_signal[:,lead]\n",
        "        \n",
        "        # exclude the nonbeats  \n",
        "        #df_ann = pd.DataFrame({'atr_sym':atr_sym,\n",
        "         #                       'atr_sample':atr_sample})\n",
        "        #df_ann = df_ann.loc[df_ann.atr_sym.isin(abnormal + ['N'])]\n",
        "            \n",
        "        begin_window = 0 #point at which we start/end a window (of length num_sec seconds)\n",
        "\n",
        "        while begin_window  < len(p_signal_lead):\n",
        "            \n",
        "            end_window = begin_window + window_width\n",
        "            \n",
        "            r_in_window = np.where((atr_sample >= begin_window) & (atr_sample < end_window)) #indices of R peaks in the window\n",
        "            \n",
        "            # we can quickly compute the segment size\n",
        "            atr_sample_window = atr_sample[r_in_window] #the R peak points of that window\n",
        "            segment_size = math.ceil(1.2*np.median(atr_sample_window[1:] - np.roll(atr_sample_window,1)[1:])) #interval width\n",
        "\n",
        "            begin_window = end_window #we don't need begin_window for this loop anymore so we set it for the next loop\n",
        "\n",
        "            X,Y = build_XY(p_signal_lead,segment_size,atr_sample,atr_sym,r_in_window, num_cols)\n",
        "            \n",
        "            X_all = np.append(X_all,X,axis = 0)\n",
        "            Y_all = Y_all+Y\n",
        "            \n",
        "                \n",
        "    # drop the first zero row\n",
        "    X_all = X_all[1:,:]\n",
        "\n",
        "    return X_all, Y_all \n",
        "\n",
        "######################\n",
        "\n",
        "\n",
        "def build_XY(p_signal_lead,segment_size,atr_sample,atr_sym,r_in_window, num_cols):\n",
        "    # this function builds the X,Y matrices for each beat\n",
        "    # it also returns the original symbols for Y\n",
        "    \n",
        "    num_rows = len(r_in_window[0]) # that r_in_window is a tuple (ordered pair)\n",
        "    \n",
        "    X = np.zeros((num_rows, num_cols))\n",
        "    Y = []\n",
        "\n",
        "    for index in range(num_rows):\n",
        "        \n",
        "        r_in_window_index = r_in_window[0][index]\n",
        "        \n",
        "        left = atr_sample[r_in_window_index]\n",
        "        right = left + segment_size\n",
        "        \n",
        "        step = len(p_signal_lead[left: right]) # is equal to segment_size until the remainder at the end of the window\n",
        "\n",
        "        # put the [R,R+1.2*T] interval put as a row in X\n",
        "        signal_vector = p_signal_lead[left: right]\n",
        "        X[index,:step] =  (signal_vector - signal_vector.min())/(signal_vector.max() - signal_vector.min()) # you may chose not to make that scaling/shifting if your modeling puposes do require\n",
        "        \n",
        "        Y += [atr_sym[r_in_window_index]]\n",
        "\n",
        "\n",
        "    return X,Y "
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "id": "MfQ0UIMd6Zto"
      },
      "source": [
        "num_sec = 10\n",
        "fs = 257\n",
        "lead = 0\n",
        "num_cols =440\n",
        "\n",
        "X_all, Y_all = make_dataset(num_cols,pts,lead, num_sec, fs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "66GfkIL46Zto"
      },
      "source": [
        "symm = np.array(Y_all)\n",
        "symm = symm.reshape(symm.shape[0],1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "afRDS0aV6Ztp"
      },
      "source": [
        "Table = np.hstack((X_all,symm))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pFLTNM886Ztp"
      },
      "source": [
        "dff = pd.DataFrame(Table)\n",
        "dff.to_csv('INCART_Lead0_with_gains.csv', index = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-phYOVE6Ztp"
      },
      "source": [
        "If the code above takes forever (huge table), or crashes the RAM, do build mini portions of it (patient 1-10, 11-20,..) then glue them together by vstack. This is how we did it (written below at the end). Also this had the benefit to confirm that the code was not taking long due to any bugs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hdDi4Z4C6Ztq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c229fc0a-67d5-4c9c-f0e4-0e4c38fe4d8a"
      },
      "source": [
        "num_sec = 10\n",
        "fs = 257\n",
        "lead = 11\n",
        "num_cols = 500\n",
        "\n",
        "X_all, Y_all = make_dataset(num_cols,pts[0:10],lead, num_sec, fs)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:78: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SYseiDoB6Ztr"
      },
      "source": [
        "X_all2, Y_all2 = make_dataset(num_cols,pts[10:20],lead, num_sec, fs)"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "21m-Huag6Ztr"
      },
      "source": [
        "X_all3, Y_all3 = make_dataset(num_cols,pts[20:30],lead, num_sec, fs)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gG8ICwS_6Ztr"
      },
      "source": [
        "X_all4, Y_all4 = make_dataset(num_cols,pts[30:40],lead, num_sec, fs)"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xAv1_JtS6Ztr"
      },
      "source": [
        "X_all5, Y_all5 = make_dataset(num_cols,pts[40:50],lead, num_sec, fs)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BBydGx9u6Ztr"
      },
      "source": [
        "X_all6, Y_all6 = make_dataset(num_cols,pts[50:60],lead, num_sec, fs)"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "irQoAxxz6Zts"
      },
      "source": [
        "X_all7, Y_all7 = make_dataset(num_cols,pts[60:70],lead, num_sec, fs)"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wAjHHABs6Zts"
      },
      "source": [
        "X_all8,  Y_all8 = make_dataset(num_cols,pts[70:],lead, num_sec, fs)"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hSIg9MHX6Zts"
      },
      "source": [
        "XX = np.vstack((X_all,X_all2))\n",
        "XX = np.vstack((XX,X_all3))\n",
        "XX = np.vstack((XX,X_all4))\n",
        "XX = np.vstack((XX,X_all5))\n",
        "XX = np.vstack((XX,X_all6))\n",
        "XX = np.vstack((XX,X_all7))\n",
        "XX = np.vstack((XX,X_all8))"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "c53tolrp6Zts"
      },
      "source": [
        "YY = np.hstack((Y_all,Y_all2))\n",
        "YY = np.hstack((YY,Y_all3))\n",
        "YY = np.hstack((YY,Y_all4))\n",
        "YY = np.hstack((YY,Y_all5))\n",
        "YY = np.hstack((YY,Y_all6))\n",
        "YY = np.hstack((YY,Y_all7))\n",
        "YY = np.hstack((YY,Y_all8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4tiOgJZ_D-5"
      },
      "source": [
        "dfX = pd.DataFrame(XX,index = None)"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtIz6lD5Bca_"
      },
      "source": [
        "dfX.to_csv('/content/drive/MyDrive/Colab Notebooks/INCART12/X_lead011.csv')"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-PE9RmTQ6Ztt"
      },
      "source": [
        "YYY= YY.reshape(YY.shape[0],1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUpDBhmA_sOt"
      },
      "source": [
        "dfY = pd.DataFrame(YYY,index = None)\r\n",
        "dfY.to_csv('/content/drive/MyDrive/Colab Notebooks/INCART12/Y_lead00.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex2_A_Wy8oKX"
      },
      "source": [
        "Table = np.hstack((XX,symmm))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DuJBF8xz6Ztt"
      },
      "source": [
        "df_shuffle.to_csv('INCART12_Lead0_shuffled.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}